{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punctuation(text):\n",
    "    '''a function for removing punctuation'''\n",
    "    import string\n",
    "    # replacing the punctuations with no space, \n",
    "    # which in effect deletes the punctuation marks \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # return the text stripped of punctuation marks\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "# extracting the stopwords from nltk library\n",
    "sw = stopwords.words('english')\n",
    "# displaying the stopwords\n",
    "np.array(sw);\n",
    "\n",
    "\n",
    "def stopwords(text):\n",
    "    '''a function for removing the stopword'''\n",
    "    # removing the stop words and lowercasing the selected words\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    # joining the list of words with space separator\n",
    "    return \" \".join(text)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stemming(text):    \n",
    "    '''a function which stems each word in the given text'''\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text) \n",
    "    \n",
    "\n",
    "\n",
    "def clean_loc(x):\n",
    "    if x == 'None':\n",
    "        return 'None'\n",
    "    elif x == 'Earth' or x =='Worldwide' or x == 'Everywhere':\n",
    "        return 'World'\n",
    "    elif 'New York' in x or 'NYC' in x:\n",
    "        return 'New York'    \n",
    "    elif 'London' in x:\n",
    "        return 'London'\n",
    "    elif 'Mumbai' in x:\n",
    "        return 'Mumbai'\n",
    "    elif 'Washington' in x and 'D' in x and 'C' in x:\n",
    "        return 'Washington DC'\n",
    "    elif 'San Francisco' in x:\n",
    "        return 'San Francisco'\n",
    "    elif 'Los Angeles' in x:\n",
    "        return 'Los Angeles'\n",
    "    elif 'Seattle' in x:\n",
    "        return 'Seattle'\n",
    "    elif 'Chicago' in x:\n",
    "        return 'Chicago'\n",
    "    elif 'Toronto' in x:\n",
    "        return 'Toronto'\n",
    "    elif 'Sacramento' in x:\n",
    "        return 'Sacramento'\n",
    "    elif 'Atlanta' in x:\n",
    "        return 'Atlanta'\n",
    "    elif 'California' in x:\n",
    "        return 'California'\n",
    "    elif 'Florida' in x:\n",
    "        return 'Florida'\n",
    "    elif 'Texas' in x:\n",
    "        return 'Texas'\n",
    "    elif 'United States' in x or 'USA' in x:\n",
    "        return 'USA'\n",
    "    elif 'United Kingdom' in x or 'UK' in x or 'Britain' in x:\n",
    "        return 'UK'\n",
    "    elif 'Canada' in x:\n",
    "        return 'Canada'\n",
    "    elif 'India' in x:\n",
    "        return 'India'\n",
    "    elif 'Kenya' in x:\n",
    "        return 'Kenya'\n",
    "    elif 'Nigeria' in x:\n",
    "        return 'Nigeria'\n",
    "    elif 'Australia' in x:\n",
    "        return 'Australia'\n",
    "    elif 'Indonesia' in x:\n",
    "        return 'Indonesia'\n",
    "    elif x in top_loc:\n",
    "        return x\n",
    "    else: return 'Others'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "\n",
    "new_df = pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "final_test = pd.read_csv('../input/nlp-getting-started/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['keyword'] = new_df['keyword'].fillna('unknown')\n",
    "new_df['location'] = new_df['location'].fillna('unknown')\n",
    "\n",
    "\n",
    "new_df = new_df[['target', 'location', 'text', 'keyword']]\n",
    "final_test = final_test[['location', 'text', 'keyword']]\n",
    "\n",
    "\n",
    "\n",
    "new_df['text'] = new_df['text'].apply(remove_punctuation)\n",
    "new_df['text'] = new_df['text'].apply(stopwords)\n",
    "new_df['text'] = new_df['text'].apply(stemming)\n",
    "new_df['text'] = new_df['text'].apply(remove_URL)\n",
    "new_df['text'] = new_df['text'].apply(remove_html)\n",
    "new_df['text'] = new_df['text'].apply(remove_emoji)\n",
    "new_df['text'] = new_df['text'].apply(remove_punct)\n",
    "\n",
    "# new_df['keyword'] = new_df['keyword'].apply(remove_punctuation)\n",
    "# new_df['keyword'] = new_df['keyword'].apply(stopwords)\n",
    "# new_df['keyword'] = new_df['keyword'].apply(stemming)\n",
    "# new_df['keyword'] = new_df['keyword'].apply(remove_URL)\n",
    "# new_df['keyword'] = new_df['keyword'].apply(remove_html)\n",
    "# new_df['keyword'] = new_df['keyword'].apply(remove_emoji)\n",
    "# new_df['keyword'] = new_df['keyword'].apply(remove_punct)\n",
    "\n",
    "\n",
    "final_test['text'] = final_test['text'].apply(remove_punctuation)\n",
    "final_test['text'] = final_test['text'].apply(stopwords)\n",
    "final_test['text'] = final_test['text'].apply(stemming)\n",
    "final_test['text'] = final_test['text'].apply(remove_URL)\n",
    "final_test['text'] = final_test['text'].apply(remove_html)\n",
    "final_test['text'] = final_test['text'].apply(remove_emoji)\n",
    "final_test['text'] = final_test['text'].apply(remove_punct)\n",
    "\n",
    "# final_test['keyword'] = final_test['keyword'].apply(remove_punctuation)\n",
    "# final_test['keyword'] = final_test['keyword'].apply(stopwords)\n",
    "# final_test['keyword'] = final_test['keyword'].apply(stemming)\n",
    "# final_test['keyword'] = final_test['keyword'].apply(remove_URL)\n",
    "# final_test['keyword'] = final_test['keyword'].apply(remove_html)\n",
    "# final_test['keyword'] = final_test['keyword'].apply(remove_emoji)\n",
    "# final_test['keyword'] = final_test['keyword'].apply(remove_punct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_loc = new_df.location.value_counts()\n",
    "top_loc = list(raw_loc[raw_loc>=10].index)\n",
    "new_df['location_clean'] = new_df['location'].apply(lambda x: clean_loc(str(x)))\n",
    "final_test['location_clean'] = final_test['location'].apply(lambda x: clean_loc(str(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['text'] = new_df['text'].apply(cleanText)\n",
    "new_df['keyword'] = new_df['keyword'].apply(cleanText)\n",
    "new_df['location_clean'] = new_df['location_clean'].apply(cleanText)\n",
    "\n",
    "final_test['text'] = final_test['text'].apply(cleanText)\n",
    "final_test['keyword'] = final_test['keyword'].fillna('unknown')\n",
    "final_test['keyword'] = final_test['keyword'].apply(cleanText)\n",
    "final_test['location_clean'] = final_test['location_clean'].apply(cleanText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_df = new_df.groupby(['keyword']).count().reset_index()\n",
    "keyword_test = final_test.groupby(['keyword']).count().reset_index()\n",
    "\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "path = get_tmpfile(\"word2vec.model\")\n",
    "model = Word2Vec(common_texts, size=100, window=1, min_count=1, workers=4)\n",
    "\n",
    "model = Word2Vec([list(keyword_df['keyword']) + list(keyword_test['keyword'])], min_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traing and test split\n",
    "train, test = train_test_split(new_df, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# another encoding technique for location and keyword, with the event rate\n",
    "keyword_val = train.groupby('keyword').agg({'target': 'mean'})\n",
    "location_val = train.groupby('location_clean').agg({'target': 'mean'})\n",
    "\n",
    "\n",
    "new_train = pd.merge(train, keyword_val, how='left', on = 'keyword')\n",
    "new_train = pd.merge(new_train, location_val, how='left', on = 'location_clean')\n",
    "\n",
    "new_test = pd.merge(test, keyword_val, how='left', on = 'keyword')\n",
    "new_test = pd.merge(new_test, location_val, how='left', on = 'location_clean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train['target_y'].fillna(new_train['target_y'].mean(), inplace=True)\n",
    "new_train['target'].fillna(new_train['target'].mean(), inplace=True)\n",
    "\n",
    "new_test['target_y'].fillna(new_train['target_y'].mean(), inplace=True)\n",
    "new_test['target'].fillna(new_train['target'].mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6090/6090 [00:00<00:00, 1091547.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# now back to creating word embeddings vector for keywords\n",
    "words = list(model.wv.vocab)\n",
    "\n",
    "train_w2v = []\n",
    "test_w2v = []\n",
    "final_test_w2v = []\n",
    "\n",
    "for elem in train['keyword']:\n",
    "    train_w2v.append(model.wv[elem])\n",
    "    \n",
    "for elem in test['keyword']:\n",
    "    test_w2v.append(model.wv[elem])\n",
    "    \n",
    "for elem in final_test['keyword']:\n",
    "    final_test_w2v.append(model.wv[elem])\n",
    "\n",
    "\n",
    "# below code to create doc2vec vector for text variable\n",
    "\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "train_tagged = train.apply(lambda r: TaggedDocument(words=tokenize_text(r['text']), tags=[r.target]), axis=1)\n",
    "test_tagged = test.apply(lambda r: TaggedDocument(words=tokenize_text(r['text']),tags=[r.target]), axis=1)\n",
    "\n",
    "\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6090/6090 [00:00<00:00, 916220.50it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1127540.89it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1045571.48it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1025094.77it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 981529.03it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1145183.20it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 984707.45it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1082206.13it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 730059.20it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1128337.81it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1043308.07it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 996967.77it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1059800.49it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 920711.94it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 933702.94it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1078596.04it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1082802.52it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1059844.46it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1069070.91it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1042031.22it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1024231.58it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1047801.76it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1104002.74it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1081335.68it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 978258.64it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1062666.36it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1036155.74it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 972522.80it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 1110240.86it/s]\n",
      "100%|██████████| 6090/6090 [00:00<00:00, 890414.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.9 s, sys: 6.06 s, total: 53.9 s\n",
      "Wall time: 40.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha\n",
    "\n",
    "\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors\n",
    "\n",
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
    "\n",
    "\n",
    "\n",
    "# now combining the doc2vec vector, with word2vec vector and keyword and location encoding \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', binary=True)\n",
    "vectorizer.fit(new_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cvec = vectorizer.transform(train['text']).todense()\n",
    "X_test_cvec = vectorizer.transform(test['text']).todense()\n",
    "\n",
    "# y = tweets['target'].values\n",
    "# X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6090, 14663), (1523, 14663))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvec.shape, X_test_cvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word', binary=True)\n",
    "tfidf.fit(new_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf.transform(train['text']).todense()\n",
    "X_test_tfidf = tfidf.transform(test['text']).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train = list(map(lambda x,y: np.append(x,y),X_train, new_train['target_y']))\n",
    "new_X_train_2 = list(map(lambda x,y: np.append(x,y),new_X_train, new_train['target']))\n",
    "new_X_train_3 = list(map(lambda x,y: np.append(x,y),new_X_train_2, train_w2v))\n",
    "\n",
    "\n",
    "new_X_test = list(map(lambda x,y: np.append(x,y),X_test, new_test['target_y']))\n",
    "new_X_test_2 = list(map(lambda x,y: np.append(x,y),new_X_test, new_test['target']))\n",
    "new_X_test_3 = list(map(lambda x,y: np.append(x,y),new_X_test_2, test_w2v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "\n",
    "\n",
    "new_X_train_4 = list(map(lambda x,y: np.append(x,y),new_X_train_3, X_train_cvec))\n",
    "new_X_test_4 = list(map(lambda x,y: np.append(x,y),new_X_test_3, X_test_cvec))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "\n",
    "new_X_train_5 = list(map(lambda x,y: np.append(x,y),new_X_train_4, X_train_tfidf))\n",
    "new_X_test_5 = list(map(lambda x,y: np.append(x,y),new_X_test_4, X_test_tfidf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_test_5[0].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy : 0.7616546290216678\n",
      "Testing F1 score : 0.7614329929675111\n"
     ]
    }
   ],
   "source": [
    "# Simple logistic regression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(new_X_train_5, y_train)\n",
    "y_pred = logreg.predict(new_X_test_5)\n",
    "print ('Testing accuracy : {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print ('Testing F1 score : {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing F1 score : 0.7553758069149518\n"
     ]
    }
   ],
   "source": [
    "var_lst = ['var_'+str(i) for i in range(len(new_X_train_4[0]))]\n",
    "\n",
    "import xgboost as xgb\n",
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'binary:logistic',\n",
    "    'silent': 1,\n",
    "    'seed' : 0,\n",
    "    'n_estimators': 200,\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "dtrain = xgb.DMatrix(new_X_train_4, y_train, feature_names=var_lst)\n",
    "xgb_model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)\n",
    "\n",
    "\n",
    "dtest = xgb.DMatrix(new_X_test_4,feature_names = var_lst )\n",
    "\n",
    "y_pred = xgb_model.predict(dtest)\n",
    "# print ('Testing accuracy : {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print ('Testing F1 score : {}'.format(f1_score(y_test, y_pred.round(), average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_all_4 = new_X_train_4+new_X_test_4\n",
    "new_y_all_4 = y_train+y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_all_4_np = np.array(new_X_all_4)\n",
    "new_y_all_4_np = np.array(new_y_all_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "models = []\n",
    "n_splits = 5\n",
    "fold = 0 \n",
    "for train_index, test_index in StratifiedKFold(n_splits=n_splits).split(new_X_all_4_np, new_y_all_4_np):\n",
    "\n",
    "    X_train, X_test = new_X_all_4_np[train_index], new_X_all_4_np[test_index]\n",
    "    y_train, y_test = new_y_all_4_np[train_index], new_y_all_4_np[test_index]\n",
    "\n",
    "    clf = LogisticRegression(max_iter=400)\n",
    "\n",
    "    clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "    models.append(clf)\n",
    "    fold += 1\n",
    "    print(fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test['target'] = 0\n",
    "\n",
    "final_test = pd.merge(final_test, keyword_val, how='left', on = 'keyword')\n",
    "final_test = pd.merge(final_test, location_val, how='left', on = 'location_clean')\n",
    "final_test['target_y'].fillna(new_train['target_y'].mean(), inplace=True)\n",
    "final_test['target'].fillna(new_train['target'].mean(), inplace=True)\n",
    "\n",
    "final_test_tagged = final_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['text']),tags=[r.target]), axis=1)\n",
    "f_y_test, f_X_test = vec_for_learning(model_dbow, final_test_tagged)\n",
    "\n",
    "final_X_test = list(map(lambda x,y: np.append(x,y), f_X_test, final_test['target_y']))\n",
    "final_X_test_2 = list(map(lambda x,y: np.append(x,y),final_X_test, final_test['target']))\n",
    "final_X_test_3 = list(map(lambda x,y: np.append(x,y),final_X_test_2, final_test_w2v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_f_test_cvec = vectorizer.transform(final_test['text']).todense()\n",
    "X_f_test_tfidf = tfidf.transform(final_test['text']).todense()\n",
    "\n",
    "\n",
    "final_X_test_4 = list(map(lambda x,y: np.append(x,y),final_X_test_3, X_f_test_cvec))\n",
    "# final_X_test_5 = list(map(lambda x,y: np.append(x,y),final_X_test_4, X_f_test_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_final_4 = list((lambda x: map(x, float), final_X_test_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1149d83f1f9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_X_test_4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "final_X_test_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_final_4_np = np.array(final_X_test_4)\n",
    "\n",
    "# new_y_all_4_np = np.array(new_y_all_4)\n",
    "# y_hat = clf.predict(new_final_4_np)\n",
    "\n",
    "final = np.zeros((new_final_4_np.shape[0]))\n",
    "\n",
    "for i in range(n_splits):\n",
    "        clf = models[i]\n",
    "        preds = clf.predict(new_final_4_np)\n",
    "        \n",
    "        final += preds/n_splits\n",
    "\n",
    "    \n",
    "final = np.where(final>=0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(final_X_test_4,feature_names = var_lst )\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([1924, 1339]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y_pred = np.where(y_pred_xgb > .45,1,0)\n",
    "unique_elements, counts_elements = np.unique(new_y_pred, return_counts=True)\n",
    "\n",
    "unique_elements, counts_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_X_test_5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-5e70d4e9f684>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_X_test_5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'final_X_test_5' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = logreg.predict(final_X_test_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the submissions file\n",
    "sub_sample = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n",
    "\n",
    "submit = sub_sample.copy()\n",
    "submit.target = final\n",
    "submit.to_csv('submit_lr_cv_convec_tfidf.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = sub_sample.copy()\n",
    "submit.target = new_y_pred\n",
    "submit.to_csv('submit_lr_cv_convec_tfidf.csv',index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
