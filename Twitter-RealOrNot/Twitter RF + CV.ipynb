{"cells":[{"metadata":{},"cell_type":"markdown","source":"After having learned from so many great people and their work on here, making my first public notebook. \n\nTested a few things\n1. Feature creation techniques for text\n2. ML algos\n\nNext up: Deep learning models, and creating cleaner notebooks"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/sample_submission.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\ntqdm.pandas(desc=\"progress-bar\")\nfrom gensim.models import Doc2Vec\nfrom sklearn import utils\nfrom sklearn.model_selection import train_test_split\nimport gensim\nfrom sklearn.linear_model import LogisticRegression\nfrom gensim.models.doc2vec import TaggedDocument\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef remove_punctuation(text):\n    '''a function for removing punctuation'''\n    import string\n    # replacing the punctuations with no space, \n    # which in effect deletes the punctuation marks \n    translator = str.maketrans('', '', string.punctuation)\n    # return the text stripped of punctuation marks\n    return text.translate(translator)\n\n\n# extracting the stopwords from nltk library\nsw = stopwords.words('english')\n# displaying the stopwords\nnp.array(sw);\n\n\ndef stopwords(text):\n    '''a function for removing the stopword'''\n    # removing the stop words and lowercasing the selected words\n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    # joining the list of words with space separator\n    return \" \".join(text)\n\nstemmer = SnowballStemmer(\"english\")\n\ndef stemming(text):    \n    '''a function which stems each word in the given text'''\n    text = [stemmer.stem(word) for word in text.split()]\n    return \" \".join(text) \n    \n\n\ndef clean_loc(x):\n    if x == 'None':\n        return 'None'\n    elif x == 'Earth' or x =='Worldwide' or x == 'Everywhere':\n        return 'World'\n    elif 'New York' in x or 'NYC' in x:\n        return 'New York'    \n    elif 'London' in x:\n        return 'London'\n    elif 'Mumbai' in x:\n        return 'Mumbai'\n    elif 'Washington' in x and 'D' in x and 'C' in x:\n        return 'Washington DC'\n    elif 'San Francisco' in x:\n        return 'San Francisco'\n    elif 'Los Angeles' in x:\n        return 'Los Angeles'\n    elif 'Seattle' in x:\n        return 'Seattle'\n    elif 'Chicago' in x:\n        return 'Chicago'\n    elif 'Toronto' in x:\n        return 'Toronto'\n    elif 'Sacramento' in x:\n        return 'Sacramento'\n    elif 'Atlanta' in x:\n        return 'Atlanta'\n    elif 'California' in x:\n        return 'California'\n    elif 'Florida' in x:\n        return 'Florida'\n    elif 'Texas' in x:\n        return 'Texas'\n    elif 'United States' in x or 'USA' in x:\n        return 'USA'\n    elif 'United Kingdom' in x or 'UK' in x or 'Britain' in x:\n        return 'UK'\n    elif 'Canada' in x:\n        return 'Canada'\n    elif 'India' in x:\n        return 'India'\n    elif 'Kenya' in x:\n        return 'Kenya'\n    elif 'Nigeria' in x:\n        return 'Nigeria'\n    elif 'Australia' in x:\n        return 'Australia'\n    elif 'Indonesia' in x:\n        return 'Indonesia'\n    elif x in top_loc:\n        return x\n    else: return 'Others'\n","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing data\n\nnew_df = pd.read_csv('../input/nlp-getting-started/train.csv')\nfinal_test = pd.read_csv('../input/nlp-getting-started/test.csv')\n","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df['keyword'] = new_df['keyword'].fillna('unknown')\nnew_df['location'] = new_df['location'].fillna('unknown')\n\n\nnew_df = new_df[['target', 'location', 'text', 'keyword']]\nfinal_test = final_test[['location', 'text', 'keyword']]\n\n\n\nnew_df['text'] = new_df['text'].apply(remove_punctuation)\nnew_df['text'] = new_df['text'].apply(stopwords)\nnew_df['text'] = new_df['text'].apply(stemming)\nnew_df['text'] = new_df['text'].apply(remove_URL)\nnew_df['text'] = new_df['text'].apply(remove_html)\nnew_df['text'] = new_df['text'].apply(remove_emoji)\nnew_df['text'] = new_df['text'].apply(remove_punct)\n\n\n\nfinal_test['text'] = final_test['text'].apply(remove_punctuation)\nfinal_test['text'] = final_test['text'].apply(stopwords)\nfinal_test['text'] = final_test['text'].apply(stemming)\nfinal_test['text'] = final_test['text'].apply(remove_URL)\nfinal_test['text'] = final_test['text'].apply(remove_html)\nfinal_test['text'] = final_test['text'].apply(remove_emoji)\nfinal_test['text'] = final_test['text'].apply(remove_punct)\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_loc = new_df.location.value_counts()\ntop_loc = list(raw_loc[raw_loc>=10].index)\nnew_df['location_clean'] = new_df['location'].apply(lambda x: clean_loc(str(x)))\nfinal_test['location_clean'] = final_test['location'].apply(lambda x: clean_loc(str(x)))\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bs4 import BeautifulSoup\n\ndef cleanText(text):\n    text = BeautifulSoup(text, \"lxml\").text\n    text = re.sub(r'\\|\\|\\|', r' ', text) \n    text = re.sub(r'http\\S+', r'<URL>', text)\n    text = text.lower()\n    text = text.replace('x', '')\n    return text\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df['text'] = new_df['text'].apply(cleanText)\nnew_df['keyword'] = new_df['keyword'].apply(cleanText)\nnew_df['location_clean'] = new_df['location_clean'].apply(cleanText)\n\nfinal_test['text'] = final_test['text'].apply(cleanText)\nfinal_test['keyword'] = final_test['keyword'].fillna('unknown')\nfinal_test['keyword'] = final_test['keyword'].apply(cleanText)\nfinal_test['location_clean'] = final_test['location_clean'].apply(cleanText)\n","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word2Vec "},{"metadata":{"trusted":true},"cell_type":"code","source":"keyword_df = new_df.groupby(['keyword']).count().reset_index()\nkeyword_test = final_test.groupby(['keyword']).count().reset_index()\n\nfrom gensim.test.utils import common_texts, get_tmpfile\nfrom gensim.models import Word2Vec\npath = get_tmpfile(\"word2vec.model\")\nmodel = Word2Vec(common_texts, size=100, window=1, min_count=1, workers=4)\n\nmodel = Word2Vec([list(keyword_df['keyword']) + list(keyword_test['keyword'])], min_count=1)\n","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical Variable Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# traing and test split\ntrain, test = train_test_split(new_df, test_size=0.2, random_state=42)\n\n\n# another encoding technique for location and keyword, with the event rate\nkeyword_val = train.groupby('keyword').agg({'target': 'mean'})\nlocation_val = train.groupby('location_clean').agg({'target': 'mean'})\n\n\nnew_train = pd.merge(train, keyword_val, how='left', on = 'keyword')\nnew_train = pd.merge(new_train, location_val, how='left', on = 'location_clean')\n\nnew_test = pd.merge(test, keyword_val, how='left', on = 'keyword')\nnew_test = pd.merge(new_test, location_val, how='left', on = 'location_clean')\n","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train['target_y'].fillna(new_train['target_y'].mean(), inplace=True)\nnew_train['target'].fillna(new_train['target'].mean(), inplace=True)\n\nnew_test['target_y'].fillna(new_train['target_y'].mean(), inplace=True)\nnew_test['target'].fillna(new_train['target'].mean(), inplace=True)\n","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now back to creating word embeddings vector for keywords\nwords = list(model.wv.vocab)\n\ntrain_w2v = []\ntest_w2v = []\nfinal_test_w2v = []\n\nfor elem in train['keyword']:\n    train_w2v.append(model.wv[elem])\n    \nfor elem in test['keyword']:\n    test_w2v.append(model.wv[elem])\n    \nfor elem in final_test['keyword']:\n    final_test_w2v.append(model.wv[elem])\n\n\n# below code to create doc2vec vector for text variable\n\nimport multiprocessing\ncores = multiprocessing.cpu_count()\n\nimport nltk\nfrom nltk.corpus import stopwords\n\ndef tokenize_text(text):\n    tokens = []\n    for sent in nltk.sent_tokenize(text):\n        for word in nltk.word_tokenize(sent):\n            if len(word) < 2:\n                continue\n            tokens.append(word.lower())\n    return tokens\n\ntrain_tagged = train.apply(lambda r: TaggedDocument(words=tokenize_text(r['text']), tags=[r.target]), axis=1)\ntest_tagged = test.apply(lambda r: TaggedDocument(words=tokenize_text(r['text']),tags=[r.target]), axis=1)\n\n\nmodel_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\nmodel_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n\n","execution_count":13,"outputs":[{"output_type":"stream","text":"100%|██████████| 6090/6090 [00:00<00:00, 1158269.23it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor epoch in range(30):\n    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n    model_dbow.alpha -= 0.002\n    model_dbow.min_alpha = model_dbow.alpha\n\n\ndef vec_for_learning(model, tagged_docs):\n    sents = tagged_docs.values\n    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n    return targets, regressors\n\ny_train, X_train = vec_for_learning(model_dbow, train_tagged)\ny_test, X_test = vec_for_learning(model_dbow, test_tagged)\n\n\n\n# now combining the doc2vec vector, with word2vec vector and keyword and location encoding \n\n\n\n","execution_count":14,"outputs":[{"output_type":"stream","text":"100%|██████████| 6090/6090 [00:00<00:00, 1046814.12it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1101099.72it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1163492.36it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1170047.70it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1164765.68it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1181357.48it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1177002.64it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1154186.95it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1174189.18it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1124710.99it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1054375.93it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1168068.02it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1153144.84it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 947171.14it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1148013.99it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 955533.12it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1131887.77it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1175161.55it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1064615.15it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1117918.13it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 801559.96it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1166095.02it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 851415.33it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 704856.96it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1178034.01it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 873066.66it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1147189.05it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 790129.65it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1161006.83it/s]\n100%|██████████| 6090/6090 [00:00<00:00, 1109421.10it/s]\n","name":"stderr"},{"output_type":"stream","text":"CPU times: user 40.4 s, sys: 5.14 s, total: 45.5 s\nWall time: 35.4 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Count Vectorizer and TFIDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nvectorizer = CountVectorizer(analyzer='word', binary=True)\nvectorizer.fit(new_df['text'])","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, vocabulary=None)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cvec = vectorizer.transform(train['text']).todense()\nX_test_cvec = vectorizer.transform(test['text']).todense()\n\n# y = tweets['target'].values\n# X.shape, y.shape","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cvec.shape, X_test_cvec.shape","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"((6090, 14663), (1523, 14663))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(analyzer='word', binary=True)\ntfidf.fit(new_df['text'])","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\n                input='content', lowercase=True, max_df=1.0, max_features=None,\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n                smooth_idf=True, stop_words=None, strip_accents=None,\n                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, use_idf=True, vocabulary=None)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_tfidf = tfidf.transform(train['text']).todense()\nX_test_tfidf = tfidf.transform(test['text']).todense()","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bringing in all the variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_X_train = list(map(lambda x,y: np.append(x,y),X_train, new_train['target_y']))\nnew_X_train_2 = list(map(lambda x,y: np.append(x,y),new_X_train, new_train['target']))\nnew_X_train_3 = list(map(lambda x,y: np.append(x,y),new_X_train_2, train_w2v))\n\n\nnew_X_test = list(map(lambda x,y: np.append(x,y),X_test, new_test['target_y']))\nnew_X_test_2 = list(map(lambda x,y: np.append(x,y),new_X_test, new_test['target']))\nnew_X_test_3 = list(map(lambda x,y: np.append(x,y),new_X_test_2, test_w2v))\n","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CountVectorizer\n\n\nnew_X_train_4 = list(map(lambda x,y: np.append(x,y),new_X_train_3, X_train_cvec))\nnew_X_test_4 = list(map(lambda x,y: np.append(x,y),new_X_test_3, X_test_cvec))\n\n\n","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TFIDF\n\nnew_X_train_5 = list(map(lambda x,y: np.append(x,y),new_X_train_4, X_train_tfidf))\nnew_X_test_5 = list(map(lambda x,y: np.append(x,y),new_X_test_4, X_test_tfidf))\n\n","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_X_test_5[0].dtype","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"dtype('float64')"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple logistic regression\n\nfrom sklearn.metrics import accuracy_score, f1_score\n\n\nlogreg = LogisticRegression(n_jobs=1, C=1e5)\nlogreg.fit(new_X_train_5, y_train)\ny_pred = logreg.predict(new_X_test_5)\nprint ('Testing accuracy : {}'.format(accuracy_score(y_test, y_pred)))\nprint ('Testing F1 score : {}'.format(f1_score(y_test, y_pred, average='weighted')))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_jobs=50, random_state=0)\nclf.fit(new_X_train_5, y_train)\n","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=100,\n                       n_jobs=50, oob_score=False, random_state=0, verbose=0,\n                       warm_start=False)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\ny_pred = clf.predict(new_X_test_5)\nprint ('Testing accuracy : {}'.format(accuracy_score(y_test, y_pred)))\nprint ('Testing F1 score : {}'.format(f1_score(y_test, y_pred, average='weighted')))\n\n","execution_count":31,"outputs":[{"output_type":"stream","text":"Testing accuracy : 0.7019041365725541\nTesting F1 score : 0.6960342579589649\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# XG Boost"},{"metadata":{"trusted":true},"cell_type":"code","source":"var_lst = ['var_'+str(i) for i in range(len(new_X_train_4[0]))]\n\nimport xgboost as xgb\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 5,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'binary:logistic',\n    'silent': 1,\n    'seed' : 0,\n    'n_estimators': 200,\n    'eval_metric': 'logloss'\n}\ndtrain = xgb.DMatrix(new_X_train_4, y_train, feature_names=var_lst)\nxgb_model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)\n\n\ndtest = xgb.DMatrix(new_X_test_4,feature_names = var_lst )\n\ny_pred = xgb_model.predict(dtest)\n# print ('Testing accuracy : {}'.format(accuracy_score(y_test, y_pred)))\nprint ('Testing F1 score : {}'.format(f1_score(y_test, y_pred.round(), average='weighted')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression - Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_X_all_4 = new_X_train_4+new_X_test_4\nnew_y_all_4 = y_train+y_test","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_X_all_4_np = np.array(new_X_all_4)\nnew_y_all_4_np = np.array(new_y_all_4)","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nmodels = []\nn_splits = 5\nfold = 0 \nfor train_index, test_index in StratifiedKFold(n_splits=n_splits).split(new_X_all_4_np, new_y_all_4_np):\n\n    X_train, X_test = new_X_all_4_np[train_index], new_X_all_4_np[test_index]\n    y_train, y_test = new_y_all_4_np[train_index], new_y_all_4_np[test_index]\n\n    clf = LogisticRegression(max_iter=400)\n\n    clf.fit(X_train,y_train)\n\n\n    models.append(clf)\n    fold += 1\n    print(fold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest - Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nmodels = []\nn_splits = 5\nfold = 0 \nfor train_index, test_index in StratifiedKFold(n_splits=n_splits).split(new_X_all_4_np, new_y_all_4_np):\n\n    X_train, X_test = new_X_all_4_np[train_index], new_X_all_4_np[test_index]\n    y_train, y_test = new_y_all_4_np[train_index], new_y_all_4_np[test_index]\n\n    clf = RandomForestClassifier(n_jobs=50, random_state=0)\n\n    clf.fit(X_train,y_train)\n\n\n    models.append(clf)\n    fold += 1\n    print(fold)","execution_count":36,"outputs":[{"output_type":"stream","text":"1\n2\n3\n4\n5\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Final Test Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test['target'] = 0\n\nfinal_test = pd.merge(final_test, keyword_val, how='left', on = 'keyword')\nfinal_test = pd.merge(final_test, location_val, how='left', on = 'location_clean')\nfinal_test['target_y'].fillna(new_train['target_y'].mean(), inplace=True)\nfinal_test['target'].fillna(new_train['target'].mean(), inplace=True)\n\nfinal_test_tagged = final_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['text']),tags=[r.target]), axis=1)\nf_y_test, f_X_test = vec_for_learning(model_dbow, final_test_tagged)\n\nfinal_X_test = list(map(lambda x,y: np.append(x,y), f_X_test, final_test['target_y']))\nfinal_X_test_2 = list(map(lambda x,y: np.append(x,y),final_X_test, final_test['target']))\nfinal_X_test_3 = list(map(lambda x,y: np.append(x,y),final_X_test_2, final_test_w2v))\n","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_f_test_cvec = vectorizer.transform(final_test['text']).todense()\nX_f_test_tfidf = tfidf.transform(final_test['text']).todense()\n\n\nfinal_X_test_4 = list(map(lambda x,y: np.append(x,y),final_X_test_3, X_f_test_cvec))\n# final_X_test_5 = list(map(lambda x,y: np.append(x,y),final_X_test_4, X_f_test_tfidf))","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_final_4 = list((lambda x: map(x, float), final_X_test_4))","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_final_4_np = np.array(final_X_test_4)\n\n# new_y_all_4_np = np.array(new_y_all_4)\n# y_hat = clf.predict(new_final_4_np)\n\nfinal = np.zeros((new_final_4_np.shape[0]))\n\nfor i in range(n_splits):\n        clf = models[i]\n        preds = clf.predict(new_final_4_np)\n        \n        final += preds/n_splits\n\n    \nfinal = np.where(final>=0.5,1,0)","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgboost\n\n# dtest = xgb.DMatrix(final_X_test_4,feature_names = var_lst )\n\n# y_pred_xgb = xgb_model.predict(dtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating the submissions file\nsub_sample = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n\nsubmit = sub_sample.copy()\nsubmit.target = final\nsubmit.to_csv('submit_rf_cv.csv',index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Please let me know if there are any questions. And also, would appreciate an upvote if you found the notebook helpful.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}